{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNCeY_niioep"
   },
   "source": [
    "Detailed explanation:\n",
    "https://medium.com/@alexppppp/how-to-train-a-custom-keypoint-detection-model-with-pytorch-d9af90e111da\n",
    "\n",
    "GitHub repo:\n",
    "https://github.com/alexppppp/keypoint_rcnn_training_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GmCpY-Hioer"
   },
   "source": [
    "\n",
    "# CUHK Jockey Club AI for the Future Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:Set up Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries and Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "import albumentations as A  # Library for augmentations\n",
    "\n",
    "# https://github.com/pytorch/vision/tree/main/references/detection\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_and_copy_files(source_directory, train_directory, test_directory, train_count=15, test_count=5):\n",
    "    \"\"\"\n",
    "    Rename and copy files from the source directory to train and test directories.\n",
    "\n",
    "    Parameters:\n",
    "    source_directory (str): Path to the source directory containing the raw photos.\n",
    "    train_directory (str): Path to the directory where the training images will be copied.\n",
    "    test_directory (str): Path to the directory where the testing images will be copied.\n",
    "    train_count (int): Number of files to be used for training. Default is 15.\n",
    "    test_count (int): Number of files to be used for testing. Default is 5.\n",
    "    \"\"\"\n",
    "    \n",
    "    # List all files in the source directory\n",
    "    files = os.listdir(source_directory)\n",
    "    \n",
    "    # Check if there are enough files in the source directory\n",
    "    if len(files) < train_count + test_count:\n",
    "        raise ValueError(\"Not enough files in the source directory to allocate to train and test sets\")\n",
    "      \n",
    "    # Select files for training and testing\n",
    "    test_files = files[:test_count]\n",
    "    train_files = files[test_count:train_count + test_count]\n",
    "    \n",
    "    # Clear all files in the train directory\n",
    "    for filename in os.listdir(train_directory):\n",
    "        file_path = os.path.join(train_directory, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "    \n",
    "    # Clear all files in the test directory\n",
    "    for filename in os.listdir(test_directory):\n",
    "        file_path = os.path.join(test_directory, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "    \n",
    "    # Rename and copy training files to the train directory\n",
    "    for i, filename in enumerate(train_files):\n",
    "        new_name = f\"train_{i+1}{os.path.splitext(filename)[1]}\"\n",
    "        source_file = os.path.join(source_directory, filename)\n",
    "        destination_file = os.path.join(train_directory, new_name)\n",
    "        shutil.copy2(source_file, destination_file)\n",
    "        print(f\"Copied: {source_file} to {destination_file}\")\n",
    "    \n",
    "    # Rename and copy testing files to the test directory\n",
    "    for i, filename in enumerate(test_files):\n",
    "        new_name = f\"test_{i+1}{os.path.splitext(filename)[1]}\"\n",
    "        source_file = os.path.join(source_directory, filename)\n",
    "        destination_file = os.path.join(test_directory, new_name)\n",
    "        shutil.copy2(source_file, destination_file)\n",
    "        print(f\"Copied: {source_file} to {destination_file}\")\n",
    "\n",
    "def clear_directory(directory):\n",
    "    \"\"\"\n",
    "    Delete all files and subdirectories in the specified directory, but keep the directory itself.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): Path to the directory to be cleared.\n",
    "    \"\"\"\n",
    "    if os.path.exists(directory):\n",
    "        for filename in os.listdir(directory):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)\n",
    "                    print(f\"Deleted file: {file_path}\")\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)\n",
    "                    print(f\"Deleted directory: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {directory}\")\n",
    "\n",
    "        \n",
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more here https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOhs005gjHzQ",
    "outputId": "1afbace4-71d1-4e87-e38b-a1f3045bc5c9"
   },
   "outputs": [],
   "source": [
    "\n",
    "!git clone https://github.com/Camellia-k27/Simple_Keypoint_Detect.git\n",
    "%cd Simple_Keypoint_Detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Upload images of objects for detection as training data (Optional)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the paths to the source and destination directories\n",
    "source_directory = \"my_set/raw_photos\"\n",
    "train_directory = \"my_set/train/images\"\n",
    "test_directory = \"my_set/test/images\"\n",
    "\n",
    "# Call the function to rename and copy files\n",
    "rename_and_copy_files(source_directory, train_directory, test_directory)\n",
    "\n",
    "# Create zip archives of the train and test directories\n",
    "shutil.make_archive('train', 'zip', train_directory)\n",
    "shutil.make_archive('test', 'zip', test_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Delete the original train and test datasets (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the paths to the directories to be cleared\n",
    "train_labels_directory = \"my_set/train/labels\"\n",
    "test_labels_directory = \"my_set/test/labels\"\n",
    "\n",
    "# Call the function to clear the directories\n",
    "clear_directory(train_labels_directory)\n",
    "clear_directory(test_labels_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6kvhJZbg-Jn"
   },
   "source": [
    "\n",
    "# Step 4: Hand-Label Training Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4vNy5N8gB1o"
   },
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "output.serve_kernel_port_as_window(5000)\n",
    "!python /content/Simple_Keypoint_Detect/labeller/labeller.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3sEXMIOioes"
   },
   "source": [
    "# Step 5: Categorize and Process Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lavrsk51ioet"
   },
   "outputs": [],
   "source": [
    "class ClassDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"labels\")))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"labels\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            #check if the bboxes is out of the image\n",
    "            for i in range(len(bboxes_original)):\n",
    "                bboxes_original[i][0] = max(0, bboxes_original[i][0])\n",
    "                bboxes_original[i][1] = max(0, bboxes_original[i][1])\n",
    "                bboxes_original[i][2] = min(img_original.shape[1], bboxes_original[i][2])\n",
    "                bboxes_original[i][3] = min(img_original.shape[0], bboxes_original[i][3])\n",
    "            # All objects are glue tubes\n",
    "            bboxes_labels_original = ['Glue tube' for _ in bboxes_original]\n",
    "\n",
    "        if self.transform:\n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "\n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,2,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened): # Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    # kp - coordinates of keypoint\n",
    "                    # keypoints_original[o_idx][k_idx][2] - original visibility of keypoint\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "\n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original\n",
    "\n",
    "        # Convert everything into a torch tensor\n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor([1 for _ in bboxes], dtype=torch.int64) # all objects are glue tubes\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)\n",
    "\n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor([1 for _ in bboxes_original], dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)\n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original\n",
    "        else:\n",
    "            return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYqFcpIaioet"
   },
   "source": [
    "# Step 6: Visualize Random Labeled Sample \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "id": "C3Tle7zhioeu",
    "outputId": "77e515ce-ab45-44d7-c28f-3cc2c793c743"
   },
   "outputs": [],
   "source": [
    "KEYPOINTS_FOLDER_TRAIN = 'my_set/test'\n",
    "#KEYPOINTS_FOLDER_TRAIN = 'my_set/train_blocked'\n",
    "#KEYPOINTS_FOLDER_TRAIN = 'my_set/test_blurred'\n",
    "dataset = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=True)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "iterator = iter(data_loader)\n",
    "batch = next(iterator)\n",
    "\n",
    "print(\"Original targets:\\n\", batch[3], \"\\n\\n\")\n",
    "print(\"Transformed targets:\\n\", batch[1])\n",
    "\n",
    "keypoints_classes_ids2names = {0: 'mouse', 1: 'pen'} \n",
    "# keypoints_classes_ids2names = {0: 'cat', 1: 'dog'} according to the dataset\n",
    "\n",
    "def visualize(image, bboxes, keypoints, image_original=None, bboxes_original=None, keypoints_original=None):\n",
    "    fontsize = 13\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        start_point = (bbox[0], bbox[1])\n",
    "        end_point = (bbox[2], bbox[3])\n",
    "        image = cv2.rectangle(image.copy(), start_point, end_point, (0,255,0), 1)\n",
    "\n",
    "    for kps in keypoints:\n",
    "        n=0\n",
    "        for idx, kp in enumerate(kps):\n",
    "          if n ==0:\n",
    "            image = cv2.circle(image.copy(), tuple(kp), 2, (255,0,0), 2)\n",
    "            image = cv2.putText(image.copy(), \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_PLAIN, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "          else:\n",
    "            image = cv2.circle(image.copy(), tuple(kp), 2, (0,0,255), 2)\n",
    "            image = cv2.putText(image.copy(), \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_PLAIN, 1, (0,0,255), 1, cv2.LINE_AA)\n",
    "          n+=1\n",
    "\n",
    "    if image_original is None and keypoints_original is None:\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(image)\n",
    "\n",
    "    else:\n",
    "        for bbox in bboxes_original:\n",
    "            start_point = (bbox[0], bbox[1])\n",
    "            end_point = (bbox[2], bbox[3])\n",
    "            image_original = cv2.rectangle(image_original.copy(), start_point, end_point, (0,255,0), 2)\n",
    "\n",
    "        for kps in keypoints_original:\n",
    "            n = 0\n",
    "            for idx, kp in enumerate(kps):\n",
    "              if n == 0:\n",
    "                image_original = cv2.circle(image_original, tuple(kp), 2, (255,0,0), 2)\n",
    "                image_original = cv2.putText(image_original, \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_PLAIN, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "              else:\n",
    "                image_original = cv2.circle(image_original, tuple(kp), 2, (0,0,255), 2)\n",
    "                image_original = cv2.putText(image_original, \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_PLAIN, 1, (0,0,255), 1, cv2.LINE_AA)\n",
    "              n+=1\n",
    "\n",
    "        f, ax = plt.subplots(1, 2, figsize=(40, 20))\n",
    "\n",
    "        ax[0].imshow(image_original)\n",
    "        ax[0].set_title('Original image', fontsize=fontsize)\n",
    "\n",
    "        ax[1].imshow(image)\n",
    "        ax[1].set_title('Transformed image', fontsize=fontsize)\n",
    "\n",
    "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints = []\n",
    "for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints.append([kp[:2] for kp in kps])\n",
    "\n",
    "image_original = (batch[2][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes_original = batch[3][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints_original = []\n",
    "for kps in batch[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints_original.append([kp[:2] for kp in kps])\n",
    "\n",
    "visualize(image, bboxes, keypoints, image_original, bboxes_original, keypoints_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gbrc-Qavioeu"
   },
   "source": [
    "# Step 7: Fine-Tune Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Zb0kCmIZioev",
    "outputId": "d6ef5e30-b8ca-4240-fcf8-d9d87c80d677"
   },
   "outputs": [],
   "source": [
    "def get_model(num_keypoints, weights_path=None):\n",
    "\n",
    "    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n",
    "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,\n",
    "                                                                   pretrained_backbone=True,\n",
    "                                                                   num_keypoints=num_keypoints,\n",
    "                                                                   num_classes = 2, # Background is the first class, object is the second class\n",
    "                                                                   rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "##########cells\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = 'my_set/train'\n",
    " # Change the dataset to others (train_blocked/ train_blurred) to see the changes!\n",
    "KEYPOINTS_FOLDER_TEST = 'my_set/test'\n",
    " # Change the dataset to others (test_blocked/ test_blurred) to see the changes!\n",
    "dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = get_model(num_keypoints = 2)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.3)\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=1000)\n",
    "    lr_scheduler.step()\n",
    "    evaluate(model, data_loader_test, device)\n",
    "\n",
    "# Save model weights after training\n",
    "torch.save(model.state_dict(), 'keypointsrcnn_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "typA9tSFfy1Z"
   },
   "source": [
    "# Step 8: Download Pre-trained Model(Optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBYqnbhIgqeA"
   },
   "source": [
    "If you have problem with fine-tuning the model, you can download an existing model and still make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TFVYFLcfvUl"
   },
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/XBLUECATX/KeypointRCNN_Pretrained/resolve/main/pretrained_weights.pth\n",
    "\n",
    "def load_model(num_keypoints, weights_path=None):\n",
    "\n",
    "    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n",
    "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(\n",
    "                                      pretrained=False,\n",
    "                                      pretrained_backbone=True,\n",
    "                                      num_keypoints=num_keypoints,\n",
    "                                      num_classes = 2, # Background is the first class, object is the second class\n",
    "                                      rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path,map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = load_model(num_keypoints = 2,weights_path='pretrained_weights.pth')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQFThQ0cioev"
   },
   "source": [
    "# Step 9: Visualize Model Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ue2hTAuYioev"
   },
   "outputs": [],
   "source": [
    "KEYPOINTS_FOLDER_TEST = 'my_set/test'\n",
    " # Change the dataset to others (test_blocked/ test_blurred) to see the changes!\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "iterator = iter(data_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mao8FDHSioev",
    "outputId": "c0f182d9-6efd-4c3a-fb6d-f2c1c7e887a3"
   },
   "outputs": [],
   "source": [
    "images, targets = next(iterator)\n",
    "images = list(image.to(device) for image in images)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    output = model(images)\n",
    "\n",
    "#print(\"Predictions: \\n\", output)\n",
    "\n",
    "############cells\n",
    "\n",
    "image = (images[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "scores = output[0]['scores'].detach().cpu().numpy()\n",
    "print(scores)\n",
    "\n",
    "high_scores_idxs = np.where(scores > 0.5)[0].tolist() # Indexes of boxes with scores > 0.5\n",
    "post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "keypoints = []\n",
    "for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "    if len(keypoints) != 0: #shows only best option\n",
    "      break\n",
    "\n",
    "bboxes = []\n",
    "for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    bboxes.append(list(map(int, bbox.tolist())))\n",
    "    if len(bboxes) != 0: #shows only best option\n",
    "      break\n",
    "\n",
    "print(bboxes)\n",
    "\n",
    "visualize(image, bboxes, keypoints)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
